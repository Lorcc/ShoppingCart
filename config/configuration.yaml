behaviors:
  MoveToGoal:
    trainer_type: ppo
    hyperparameters:
      batch_size: 128                   ### changed cause we are using more continuous action(default = 1024) ###  
      buffer_size: 1024                 ### changed because of the above(default = 10240) ###
      learning_rate: 0.0003             #(default = 3e-4) Typical range: 1e-5 - 1e-3. This should typically be decreased if training is unstable, and the reward does not consistently increase. 
      beta: 0.03                        #Typical range: 1e-4 - 1e-2. This should be adjusted such that the entropy (measurable from TensorBoard) slowly decreases alongside increases in reward. If entropy drops too quickly, increase beta. If entropy drops too slowly, decrease beta. 
      epsilon: 0.2                      #Typical range: 0.1 - 0.3. Setting this value small will result in more stable updates, but will also slow the training process. 
      lambd: 0.95                       #(default = 0.95) Typical range: 0.9 - 0.95. Low values correspond to relying more on the current value estimate (which can be high bias), and high values correspond to relying more on the actual rewards received in the environment (which can be high variance). 
      num_epoch: 3                      #(default = 3) Typical range: 3 - 10. Number of passes to make through the experience buffer when performing gradient descent optimization.The larger the batch_size, the larger it is acceptable to make this. Decreasing this will ensure more stable updates, at the cost of slower learning. 
      learning_rate_schedule: linear    #(default = linear for PPO and constant for SAC)
      #beta_schedule: constant           #(default = learning_rate_schedule) Determines how beta changes over time. linear decays beta linearly, reaching 0 at max_steps, while constant keeps beta constant for the entire training run.
      #epsilon_schedule: linear          #(default = learning_rate_schedule ) Determines how epsilon changes over time (PPO only). 
    network_settings:
      normalize: false                  #Normalization can be helpful in cases with complex continuous control problems, but may be harmful with simpler discrete control problems.
      hidden_units: 256                 ### more hidden Layer because the task is more complex than the average mlagent task(default = 128) ###
      num_layers: 2                     #Typical range: 1 - 3
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.99                     #(default = 0.99) Typical range: 0.8 - 0.995.  In situations when the agent should be acting in the present in order to prepare for rewards in the distant future, this value should be large. In cases when rewards are more immediate, it can be smaller. Must be strictly smaller than 1. 
        strength: 1.0                   #(default = 1.0) Factor by which to multiply the reward given by the environment. Typical ranges will vary depending on the reward signal. 
    keep_checkpoints: 5
    max_steps: 4000000
    time_horizon: 64                    ###changed to an higher value because the robot has to walk around and find the Goal first which could take some time(default = 64) ###
    summary_freq: 50000                 #(default = 50000). Number of experiences that needs to be collected before generating and displaying training statistics. This determines the granularity of the graphs in Tensorboard.
   

